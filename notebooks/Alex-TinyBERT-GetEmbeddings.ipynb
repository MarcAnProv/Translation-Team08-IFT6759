{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Get word embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "from transformers import TFBertForMaskedLM, BertConfig\n",
    "class bert_with_mask(tf.keras.Model):\n",
    "    def __init__(self, config, onehot_mask):\n",
    "        super(bert_with_mask, self).__init__()\n",
    "        self.bert = TFBertForMaskedLM(config)\n",
    "        self.onehot_mask = onehot_mask\n",
    "\n",
    "    def call(self, inputs):\n",
    "        mask = inputs[-1] # unpack mask from inputs\n",
    "        inputs = inputs[:-1]\n",
    "        outputs = self.bert(inputs)[0]\n",
    "        \n",
    "        outputs = tf.where(mask[:,:,None], outputs, self.onehot_mask[None,None,:])\n",
    "        \n",
    "        return (outputs,)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "I0413 16:40:40.681236 47228017591296 configuration_utils.py:281] loading configuration file tokenizer_data_fr_30k/config.json\n",
      "I0413 16:40:40.682730 47228017591296 configuration_utils.py:319] Model config RobertaConfig {\n",
      "  \"_num_labels\": 2,\n",
      "  \"architectures\": null,\n",
      "  \"attention_probs_dropout_prob\": 0.1,\n",
      "  \"bad_words_ids\": null,\n",
      "  \"bos_token_id\": 0,\n",
      "  \"decoder_start_token_id\": null,\n",
      "  \"do_sample\": false,\n",
      "  \"early_stopping\": false,\n",
      "  \"eos_token_id\": 2,\n",
      "  \"finetuning_task\": null,\n",
      "  \"hidden_act\": \"gelu\",\n",
      "  \"hidden_dropout_prob\": 0.1,\n",
      "  \"hidden_size\": 768,\n",
      "  \"id2label\": {\n",
      "    \"0\": \"LABEL_0\",\n",
      "    \"1\": \"LABEL_1\"\n",
      "  },\n",
      "  \"initializer_range\": 0.02,\n",
      "  \"intermediate_size\": 3072,\n",
      "  \"is_decoder\": false,\n",
      "  \"is_encoder_decoder\": false,\n",
      "  \"label2id\": {\n",
      "    \"LABEL_0\": 0,\n",
      "    \"LABEL_1\": 1\n",
      "  },\n",
      "  \"layer_norm_eps\": 1e-12,\n",
      "  \"length_penalty\": 1.0,\n",
      "  \"max_length\": 20,\n",
      "  \"max_position_embeddings\": 512,\n",
      "  \"min_length\": 0,\n",
      "  \"model_type\": \"roberta\",\n",
      "  \"no_repeat_ngram_size\": 0,\n",
      "  \"num_attention_heads\": 12,\n",
      "  \"num_beams\": 1,\n",
      "  \"num_hidden_layers\": 12,\n",
      "  \"num_return_sequences\": 1,\n",
      "  \"output_attentions\": false,\n",
      "  \"output_hidden_states\": false,\n",
      "  \"output_past\": true,\n",
      "  \"pad_token_id\": 1,\n",
      "  \"prefix\": null,\n",
      "  \"pruned_heads\": {},\n",
      "  \"repetition_penalty\": 1.0,\n",
      "  \"task_specific_params\": null,\n",
      "  \"temperature\": 1.0,\n",
      "  \"top_k\": 50,\n",
      "  \"top_p\": 1.0,\n",
      "  \"torchscript\": false,\n",
      "  \"type_vocab_size\": 2,\n",
      "  \"use_bfloat16\": false,\n",
      "  \"vocab_size\": 30000\n",
      "}\n",
      "\n",
      "I0413 16:40:40.683660 47228017591296 tokenization_utils.py:420] Model name 'tokenizer_data_fr_30k' not found in model shortcut name list (roberta-base, roberta-large, roberta-large-mnli, distilroberta-base, roberta-base-openai-detector, roberta-large-openai-detector). Assuming 'tokenizer_data_fr_30k' is a path, a model identifier, or url to a directory containing tokenizer files.\n",
      "I0413 16:40:40.690914 47228017591296 tokenization_utils.py:449] Didn't find file tokenizer_data_fr_30k/added_tokens.json. We won't load it.\n",
      "I0413 16:40:40.694400 47228017591296 tokenization_utils.py:449] Didn't find file tokenizer_data_fr_30k/tokenizer_config.json. We won't load it.\n",
      "I0413 16:40:40.695448 47228017591296 tokenization_utils.py:502] loading file tokenizer_data_fr_30k/vocab.json\n",
      "I0413 16:40:40.696306 47228017591296 tokenization_utils.py:502] loading file tokenizer_data_fr_30k/merges.txt\n",
      "I0413 16:40:40.697187 47228017591296 tokenization_utils.py:502] loading file None\n",
      "I0413 16:40:40.698011 47228017591296 tokenization_utils.py:502] loading file tokenizer_data_fr_30k/special_tokens_map.json\n",
      "I0413 16:40:40.698827 47228017591296 tokenization_utils.py:502] loading file None\n"
     ]
    }
   ],
   "source": [
    "import sys\n",
    "sys.path.insert(1, 'code')\n",
    "import pretrained_tokenizer\n",
    "\n",
    "t_fr = pretrained_tokenizer.Tokenizer(language='fr', path='tokenizer_data_fr_30k')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "onehot_mask = np.zeros(len(t_fr.tokenizer), dtype=np.float32)\n",
    "onehot_mask[t_fr.tokenizer.pad_token_id] = 1.0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "I0413 16:41:04.660619 47228017591296 configuration_utils.py:281] loading configuration file code/bert_config_tiny.json\n",
      "I0413 16:41:04.662433 47228017591296 configuration_utils.py:319] Model config BertConfig {\n",
      "  \"_num_labels\": 2,\n",
      "  \"architectures\": null,\n",
      "  \"attention_probs_dropout_prob\": 0.1,\n",
      "  \"bad_words_ids\": null,\n",
      "  \"bos_token_id\": null,\n",
      "  \"decoder_start_token_id\": null,\n",
      "  \"do_sample\": false,\n",
      "  \"early_stopping\": false,\n",
      "  \"eos_token_id\": null,\n",
      "  \"finetuning_task\": null,\n",
      "  \"hidden_act\": \"gelu\",\n",
      "  \"hidden_dropout_prob\": 0.1,\n",
      "  \"hidden_size\": 128,\n",
      "  \"id2label\": {\n",
      "    \"0\": \"LABEL_0\",\n",
      "    \"1\": \"LABEL_1\"\n",
      "  },\n",
      "  \"initializer_range\": 0.02,\n",
      "  \"intermediate_size\": 512,\n",
      "  \"is_decoder\": false,\n",
      "  \"is_encoder_decoder\": false,\n",
      "  \"label2id\": {\n",
      "    \"LABEL_0\": 0,\n",
      "    \"LABEL_1\": 1\n",
      "  },\n",
      "  \"layer_norm_eps\": 1e-12,\n",
      "  \"length_penalty\": 1.0,\n",
      "  \"max_length\": 20,\n",
      "  \"max_position_embeddings\": 512,\n",
      "  \"min_length\": 0,\n",
      "  \"model_type\": \"bert\",\n",
      "  \"no_repeat_ngram_size\": 0,\n",
      "  \"num_attention_heads\": 2,\n",
      "  \"num_beams\": 1,\n",
      "  \"num_hidden_layers\": 2,\n",
      "  \"num_return_sequences\": 1,\n",
      "  \"output_attentions\": false,\n",
      "  \"output_hidden_states\": false,\n",
      "  \"output_past\": true,\n",
      "  \"pad_token_id\": 0,\n",
      "  \"prefix\": null,\n",
      "  \"pruned_heads\": {},\n",
      "  \"repetition_penalty\": 1.0,\n",
      "  \"task_specific_params\": null,\n",
      "  \"temperature\": 1.0,\n",
      "  \"top_k\": 50,\n",
      "  \"top_p\": 1.0,\n",
      "  \"torchscript\": false,\n",
      "  \"type_vocab_size\": 2,\n",
      "  \"use_bfloat16\": false,\n",
      "  \"vocab_size\": 30522\n",
      "}\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Recreate the model\n",
    "config = BertConfig.from_pretrained('code/bert_config_tiny.json')\n",
    "config.vocab_size = len(t_fr.tokenizer)\n",
    "\n",
    "new_model = bert_with_mask(config, onehot_mask)\n",
    "optimizer = tf.keras.optimizers.Adam(learning_rate=3e-3, epsilon=1e-08, clipnorm=1.0)\n",
    "loss = tf.keras.losses.SparseCategoricalCrossentropy(from_logits=True)\n",
    "new_model.compile(optimizer=optimizer, loss=loss)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/localscratch/guest139.358248.0/lib/python3.7/site-packages/tensorflow_core/python/framework/indexed_slices.py:433: UserWarning: Converting sparse IndexedSlices to a dense Tensor of unknown shape. This may consume a large amount of memory.\n",
      "  \"Converting sparse IndexedSlices to a dense Tensor of unknown shape. \"\n",
      "W0413 16:41:28.132115 47228017591296 optimizer_v2.py:1043] Gradients do not exist for variables ['bert_with_mask/tf_bert_for_masked_lm/bert/pooler/dense/kernel:0', 'bert_with_mask/tf_bert_for_masked_lm/bert/pooler/dense/bias:0'] when minimizing the loss.\n",
      "/localscratch/guest139.358248.0/lib/python3.7/site-packages/tensorflow_core/python/framework/indexed_slices.py:433: UserWarning: Converting sparse IndexedSlices to a dense Tensor of unknown shape. This may consume a large amount of memory.\n",
      "  \"Converting sparse IndexedSlices to a dense Tensor of unknown shape. \"\n",
      "W0413 16:41:30.052045 47228017591296 optimizer_v2.py:1043] Gradients do not exist for variables ['bert_with_mask/tf_bert_for_masked_lm/bert/pooler/dense/kernel:0', 'bert_with_mask/tf_bert_for_masked_lm/bert/pooler/dense/bias:0'] when minimizing the loss.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "10.353072"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# This initializes the variables used by the optimizers,\n",
    "# as well as any stateful metric variables\n",
    "SENTENCE_LENGTH = t_fr.MAX_LENGTH\n",
    "new_model.train_on_batch((tf.zeros(shape=(1,SENTENCE_LENGTH),dtype=tf.int32), tf.ones(shape=(1,SENTENCE_LENGTH),dtype=tf.bool), tf.ones(shape=(1,SENTENCE_LENGTH),dtype=tf.bool)), (tf.zeros(shape=(1,SENTENCE_LENGTH),dtype=tf.int32)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the state of the old model\n",
    "new_model.load_weights('notebooks/ckpts/weights-improvement-20-8.78.hdf5')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[-0.03686362, -0.05840969, -0.04828245, ..., -0.04347925,\n",
       "         0.11179028, -0.05221564],\n",
       "       [-0.20925385,  0.08379307,  0.02067706, ..., -0.03660351,\n",
       "         0.08842378,  0.0282498 ],\n",
       "       [ 0.16639018, -0.19323124,  0.01101069, ...,  0.18592238,\n",
       "         0.17581482,  0.02437549],\n",
       "       ...,\n",
       "       [-0.15946755,  0.06413963, -0.05813764, ...,  0.03509144,\n",
       "         0.16632266,  0.09055192],\n",
       "       [-0.09907825, -0.07877128, -0.19373311, ..., -0.03466598,\n",
       "         0.25243932,  0.11537454],\n",
       "       [-0.24498609,  0.19010551, -0.09455546, ..., -0.05772343,\n",
       "        -0.07767264, -0.01369407]], dtype=float32)"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "weights = new_model.bert.weights[0].numpy()\n",
    "weights"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.save('tf_bert_for_masked_lm_epoch_27_loss_8.78.npy', weights)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "weights2 = np.load('tf_bert_for_masked_lm_epoch_27_loss_8.78.npy')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "assert (weights == weights2).all()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(30000, 128)"
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "weights2.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.7",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
