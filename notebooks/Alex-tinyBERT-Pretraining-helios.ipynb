{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Intialize"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Ignoring pip: markers 'python_version < \"3\"' don't match your environment\n",
      "Looking in links: /cvmfs/soft.computecanada.ca/custom/python/wheelhouse/avx, /cvmfs/soft.computecanada.ca/custom/python/wheelhouse/generic\n",
      "Processing /cvmfs/soft.computecanada.ca/custom/python/wheelhouse/generic/tensorflow_gpu-2.1.0-cp37-cp37m-linux_x86_64.whl\n",
      "Processing /cvmfs/soft.computecanada.ca/custom/python/wheelhouse/generic/numpy-1.18.2-cp37-cp37m-linux_x86_64.whl\n",
      "Processing /cvmfs/soft.computecanada.ca/custom/python/wheelhouse/generic/Keras_Preprocessing-1.1.0-py3-none-any.whl\n",
      "Requirement already satisfied: six>=1.12.0 in /opt/ipython-kernel/lib/python3.7/site-packages (from tensorflow-gpu) (1.14.0)\n",
      "Processing /cvmfs/soft.computecanada.ca/custom/python/wheelhouse/generic/grpcio-1.25.0-cp37-cp37m-linux_x86_64.whl\n",
      "Processing /cvmfs/soft.computecanada.ca/custom/python/wheelhouse/generic/termcolor-1.1.0-py3-none-any.whl\n",
      "Processing /cvmfs/soft.computecanada.ca/custom/python/wheelhouse/generic/absl_py-0.7.1-py3-none-any.whl\n",
      "Processing /cvmfs/soft.computecanada.ca/custom/python/wheelhouse/generic/Keras_Applications-1.0.8-py3-none-any.whl\n",
      "Processing /cvmfs/soft.computecanada.ca/custom/python/wheelhouse/generic/astor-0.8.1-py2.py3-none-any.whl\n",
      "Processing /cvmfs/soft.computecanada.ca/custom/python/wheelhouse/generic/wrapt-1.11.2-cp37-cp37m-linux_x86_64.whl\n",
      "Processing /cvmfs/soft.computecanada.ca/custom/python/wheelhouse/generic/tensorflow_estimator-2.1.0-py2.py3-none-any.whl\n",
      "Processing /cvmfs/soft.computecanada.ca/custom/python/wheelhouse/generic/opt_einsum-2.3.2-py3-none-any.whl\n",
      "Processing /cvmfs/soft.computecanada.ca/custom/python/wheelhouse/generic/scipy-1.4.1-cp37-cp37m-linux_x86_64.whl\n",
      "Processing /cvmfs/soft.computecanada.ca/custom/python/wheelhouse/generic/wheel-0.33.4-py2.py3-none-any.whl\n",
      "Processing /cvmfs/soft.computecanada.ca/custom/python/wheelhouse/generic/protobuf-3.11.3-py2.py3-none-any.whl\n",
      "Processing /cvmfs/soft.computecanada.ca/custom/python/wheelhouse/generic/tensorboard-2.1.0-py3-none-any.whl\n",
      "Processing /cvmfs/soft.computecanada.ca/custom/python/wheelhouse/generic/google_pasta-0.1.8-py3-none-any.whl\n",
      "Processing /cvmfs/soft.computecanada.ca/custom/python/wheelhouse/generic/gast-0.2.2-py3-none-any.whl\n",
      "Processing /cvmfs/soft.computecanada.ca/custom/python/wheelhouse/avx/h5py-2.9.0-cp37-cp37m-linux_x86_64.whl\n",
      "Requirement already satisfied: setuptools in /opt/ipython-kernel/lib/python3.7/site-packages (from protobuf>=3.8.0->tensorflow-gpu) (46.1.3)\n",
      "Processing /cvmfs/soft.computecanada.ca/custom/python/wheelhouse/generic/google_auth-1.11.0-py2.py3-none-any.whl\n",
      "Processing /cvmfs/soft.computecanada.ca/custom/python/wheelhouse/generic/Markdown-3.2-py2.py3-none-any.whl\n",
      "Processing /cvmfs/soft.computecanada.ca/custom/python/wheelhouse/generic/Werkzeug-0.16.1-py2.py3-none-any.whl\n",
      "Processing /cvmfs/soft.computecanada.ca/custom/python/wheelhouse/generic/requests-2.23.0-py2.py3-none-any.whl\n",
      "Processing /cvmfs/soft.computecanada.ca/custom/python/wheelhouse/generic/google_auth_oauthlib-0.4.1-py2.py3-none-any.whl\n",
      "Processing /cvmfs/soft.computecanada.ca/custom/python/wheelhouse/generic/rsa-4.0-py2.py3-none-any.whl\n",
      "Processing /cvmfs/soft.computecanada.ca/custom/python/wheelhouse/generic/pyasn1_modules-0.2.8-py2.py3-none-any.whl\n",
      "Processing /cvmfs/soft.computecanada.ca/custom/python/wheelhouse/generic/cachetools-3.1.1-py2.py3-none-any.whl\n",
      "Processing /cvmfs/soft.computecanada.ca/custom/python/wheelhouse/generic/chardet-3.0.4-py2.py3-none-any.whl\n",
      "Processing /cvmfs/soft.computecanada.ca/custom/python/wheelhouse/generic/certifi-2019.11.28-py2.py3-none-any.whl\n",
      "Processing /cvmfs/soft.computecanada.ca/custom/python/wheelhouse/generic/idna-2.9-py2.py3-none-any.whl\n",
      "Processing /cvmfs/soft.computecanada.ca/custom/python/wheelhouse/generic/urllib3-1.25.8-py2.py3-none-any.whl\n",
      "Processing /cvmfs/soft.computecanada.ca/custom/python/wheelhouse/generic/requests_oauthlib-1.3.0-py2.py3-none-any.whl\n",
      "Processing /cvmfs/soft.computecanada.ca/custom/python/wheelhouse/generic/pyasn1-0.4.8-py2.py3-none-any.whl\n",
      "Processing /cvmfs/soft.computecanada.ca/custom/python/wheelhouse/generic/oauthlib-3.1.0-py2.py3-none-any.whl\n",
      "Installing collected packages: numpy, keras-preprocessing, grpcio, termcolor, absl-py, h5py, keras-applications, astor, wrapt, tensorflow-estimator, opt-einsum, scipy, wheel, protobuf, pyasn1, rsa, pyasn1-modules, cachetools, google-auth, markdown, werkzeug, chardet, certifi, idna, urllib3, requests, oauthlib, requests-oauthlib, google-auth-oauthlib, tensorboard, google-pasta, gast, tensorflow-gpu\n",
      "Successfully installed absl-py-0.7.1 astor-0.8.1 cachetools-3.1.1 certifi-2019.11.28 chardet-3.0.4 gast-0.2.2 google-auth-1.11.0 google-auth-oauthlib-0.4.1 google-pasta-0.1.8 grpcio-1.25.0 h5py-2.9.0 idna-2.9 keras-applications-1.0.8 keras-preprocessing-1.1.0 markdown-3.2 numpy-1.18.2 oauthlib-3.1.0 opt-einsum-2.3.2 protobuf-3.11.3 pyasn1-0.4.8 pyasn1-modules-0.2.8 requests-2.23.0 requests-oauthlib-1.3.0 rsa-4.0 scipy-1.4.1 tensorboard-2.1.0 tensorflow-estimator-2.1.0 tensorflow-gpu-2.1.0 termcolor-1.1.0 urllib3-1.25.8 werkzeug-0.16.1 wheel-0.33.4 wrapt-1.11.2\n"
     ]
    }
   ],
   "source": [
    "!pip install --no-index tensorflow-gpu numpy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Ignoring pip: markers 'python_version < \"3\"' don't match your environment\n",
      "Looking in links: /cvmfs/soft.computecanada.ca/custom/python/wheelhouse/avx, /cvmfs/soft.computecanada.ca/custom/python/wheelhouse/generic\n",
      "Processing /home/guest139/s3transfer/botocore\n",
      "Processing /cvmfs/soft.computecanada.ca/custom/python/wheelhouse/generic/jmespath-0.9.4-py2.py3-none-any.whl\n",
      "Processing /cvmfs/soft.computecanada.ca/custom/python/wheelhouse/generic/docutils-0.15.2-py3-none-any.whl\n",
      "Requirement already satisfied: python-dateutil<3.0.0,>=2.1 in /opt/ipython-kernel/lib/python3.7/site-packages (from botocore==1.15.37) (2.8.1)\n",
      "Requirement already satisfied: urllib3<1.26,>=1.20 in /localscratch/guest139.356254.0/lib/python3.7/site-packages (from botocore==1.15.37) (1.25.8)\n",
      "Requirement already satisfied: six>=1.5 in /opt/ipython-kernel/lib/python3.7/site-packages (from python-dateutil<3.0.0,>=2.1->botocore==1.15.37) (1.14.0)\n",
      "Building wheels for collected packages: botocore\n",
      "  Building wheel for botocore (setup.py) ... \u001b[?25ldone\n",
      "\u001b[?25h  Created wheel for botocore: filename=botocore-1.15.37-py2.py3-none-any.whl size=6074726 sha256=f6586f3520f336e9bacbb123f094d249d4ae921d2a38e170e06b079b139f0263\n",
      "  Stored in directory: /tmp/pip-ephem-wheel-cache-lcp7ippk/wheels/e9/ee/87/770ad509ddd67e7e26c3442d1787625e278440f1a11c95d7ec\n",
      "Successfully built botocore\n",
      "Installing collected packages: jmespath, docutils, botocore\n",
      "Successfully installed botocore-1.15.37 docutils-0.15.2 jmespath-0.9.4\n",
      "Ignoring pip: markers 'python_version < \"3\"' don't match your environment\n",
      "Looking in links: /cvmfs/soft.computecanada.ca/custom/python/wheelhouse/avx, /cvmfs/soft.computecanada.ca/custom/python/wheelhouse/generic\n",
      "Processing /home/guest139/s3transfer\n",
      "Requirement already satisfied: botocore<2.0a.0,>=1.12.36 in /localscratch/guest139.356254.0/lib/python3.7/site-packages (from s3transfer==0.3.3) (1.15.37)\n",
      "Requirement already satisfied: jmespath<1.0.0,>=0.7.1 in /localscratch/guest139.356254.0/lib/python3.7/site-packages (from botocore<2.0a.0,>=1.12.36->s3transfer==0.3.3) (0.9.4)\n",
      "Requirement already satisfied: python-dateutil<3.0.0,>=2.1 in /opt/ipython-kernel/lib/python3.7/site-packages (from botocore<2.0a.0,>=1.12.36->s3transfer==0.3.3) (2.8.1)\n",
      "Requirement already satisfied: urllib3<1.26,>=1.20 in /localscratch/guest139.356254.0/lib/python3.7/site-packages (from botocore<2.0a.0,>=1.12.36->s3transfer==0.3.3) (1.25.8)\n",
      "Requirement already satisfied: docutils<0.16,>=0.10 in /localscratch/guest139.356254.0/lib/python3.7/site-packages (from botocore<2.0a.0,>=1.12.36->s3transfer==0.3.3) (0.15.2)\n",
      "Requirement already satisfied: six>=1.5 in /opt/ipython-kernel/lib/python3.7/site-packages (from python-dateutil<3.0.0,>=2.1->botocore<2.0a.0,>=1.12.36->s3transfer==0.3.3) (1.14.0)\n",
      "Building wheels for collected packages: s3transfer\n",
      "  Building wheel for s3transfer (setup.py) ... \u001b[?25ldone\n",
      "\u001b[?25h  Created wheel for s3transfer: filename=s3transfer-0.3.3-py2.py3-none-any.whl size=72687 sha256=e7f7339688345bfd8e32e2bfe1ac9498fdcebe5c9b2d7aa472e0f33d2e8ab1ea\n",
      "  Stored in directory: /tmp/pip-ephem-wheel-cache-72cie7mr/wheels/46/ad/ea/bdcd2d3525f19d8bb3d15b9c8e9f58087f4db77f2545998fca\n",
      "Successfully built s3transfer\n",
      "Installing collected packages: s3transfer\n",
      "Successfully installed s3transfer-0.3.3\n",
      "Ignoring pip: markers 'python_version < \"3\"' don't match your environment\n",
      "Looking in links: /cvmfs/soft.computecanada.ca/custom/python/wheelhouse/avx, /cvmfs/soft.computecanada.ca/custom/python/wheelhouse/generic\n",
      "Processing /home/guest139/sacremoses\n",
      "Processing /cvmfs/soft.computecanada.ca/custom/python/wheelhouse/generic/regex-2019.11.1-cp37-cp37m-linux_x86_64.whl\n",
      "Requirement already satisfied: six in /opt/ipython-kernel/lib/python3.7/site-packages (from sacremoses==0.0.38) (1.14.0)\n",
      "Processing /cvmfs/soft.computecanada.ca/custom/python/wheelhouse/generic/click-7.1.1-py2.py3-none-any.whl\n",
      "Processing /cvmfs/soft.computecanada.ca/custom/python/wheelhouse/generic/joblib-0.14.1-py2.py3-none-any.whl\n",
      "Processing /cvmfs/soft.computecanada.ca/custom/python/wheelhouse/generic/tqdm-4.40.2-py2.py3-none-any.whl\n",
      "Building wheels for collected packages: sacremoses\n",
      "  Building wheel for sacremoses (setup.py) ... \u001b[?25ldone\n",
      "\u001b[?25h  Created wheel for sacremoses: filename=sacremoses-0.0.38-py3-none-any.whl size=883293 sha256=77643aae8b86b268a11adb05e43aa3fab4b90642800aa9153b9c9901e8f70f2b\n",
      "  Stored in directory: /tmp/pip-ephem-wheel-cache-y7abyz40/wheels/21/1d/8d/e96ba992ac87531e2d8a03bc88ced0d0805a5f1b60ad25d369\n",
      "Successfully built sacremoses\n",
      "Installing collected packages: regex, click, joblib, tqdm, sacremoses\n",
      "Successfully installed click-7.1.1 joblib-0.14.1 regex-2019.11.1 sacremoses-0.0.38 tqdm-4.40.2\n",
      "Ignoring pip: markers 'python_version < \"3\"' don't match your environment\n",
      "Looking in links: /cvmfs/soft.computecanada.ca/custom/python/wheelhouse/avx, /cvmfs/soft.computecanada.ca/custom/python/wheelhouse/generic\n",
      "Processing /home/guest139/transformers\n",
      "Requirement already satisfied: numpy in /localscratch/guest139.356254.0/lib/python3.7/site-packages (from transformers==2.8.0) (1.18.2)\n",
      "Processing /cvmfs/soft.computecanada.ca/custom/python/wheelhouse/generic/tokenizers-0.5.2-cp37-cp37m-linux_x86_64.whl\n",
      "Processing /cvmfs/soft.computecanada.ca/custom/python/wheelhouse/generic/boto3-1.12.31-py2.py3-none-any.whl\n",
      "Processing /cvmfs/soft.computecanada.ca/custom/python/wheelhouse/generic/filelock-3.0.12-py3-none-any.whl\n",
      "Requirement already satisfied: requests in /localscratch/guest139.356254.0/lib/python3.7/site-packages (from transformers==2.8.0) (2.23.0)\n",
      "Requirement already satisfied: tqdm>=4.27 in /localscratch/guest139.356254.0/lib/python3.7/site-packages (from transformers==2.8.0) (4.40.2)\n",
      "Requirement already satisfied: regex!=2019.12.17 in /localscratch/guest139.356254.0/lib/python3.7/site-packages (from transformers==2.8.0) (2019.11.1)\n",
      "Processing /cvmfs/soft.computecanada.ca/custom/python/wheelhouse/generic/sentencepiece-0.1.82-cp37-cp37m-linux_x86_64.whl\n",
      "Requirement already satisfied: sacremoses in /localscratch/guest139.356254.0/lib/python3.7/site-packages (from transformers==2.8.0) (0.0.38)\n",
      "Requirement already satisfied: jmespath<1.0.0,>=0.7.1 in /localscratch/guest139.356254.0/lib/python3.7/site-packages (from boto3->transformers==2.8.0) (0.9.4)\n",
      "Requirement already satisfied: botocore<1.16.0,>=1.15.31 in /localscratch/guest139.356254.0/lib/python3.7/site-packages (from boto3->transformers==2.8.0) (1.15.37)\n",
      "Requirement already satisfied: s3transfer<0.4.0,>=0.3.0 in /localscratch/guest139.356254.0/lib/python3.7/site-packages (from boto3->transformers==2.8.0) (0.3.3)\n",
      "Requirement already satisfied: idna<3,>=2.5 in /localscratch/guest139.356254.0/lib/python3.7/site-packages (from requests->transformers==2.8.0) (2.9)\n",
      "Requirement already satisfied: chardet<4,>=3.0.2 in /localscratch/guest139.356254.0/lib/python3.7/site-packages (from requests->transformers==2.8.0) (3.0.4)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /localscratch/guest139.356254.0/lib/python3.7/site-packages (from requests->transformers==2.8.0) (2019.11.28)\n",
      "Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /localscratch/guest139.356254.0/lib/python3.7/site-packages (from requests->transformers==2.8.0) (1.25.8)\n",
      "Requirement already satisfied: six in /opt/ipython-kernel/lib/python3.7/site-packages (from sacremoses->transformers==2.8.0) (1.14.0)\n",
      "Requirement already satisfied: joblib in /localscratch/guest139.356254.0/lib/python3.7/site-packages (from sacremoses->transformers==2.8.0) (0.14.1)\n",
      "Requirement already satisfied: click in /localscratch/guest139.356254.0/lib/python3.7/site-packages (from sacremoses->transformers==2.8.0) (7.1.1)\n",
      "Requirement already satisfied: docutils<0.16,>=0.10 in /localscratch/guest139.356254.0/lib/python3.7/site-packages (from botocore<1.16.0,>=1.15.31->boto3->transformers==2.8.0) (0.15.2)\n",
      "Requirement already satisfied: python-dateutil<3.0.0,>=2.1 in /opt/ipython-kernel/lib/python3.7/site-packages (from botocore<1.16.0,>=1.15.31->boto3->transformers==2.8.0) (2.8.1)\n",
      "Building wheels for collected packages: transformers\n",
      "  Building wheel for transformers (setup.py) ... \u001b[?25ldone\n",
      "\u001b[?25h  Created wheel for transformers: filename=transformers-2.8.0-py3-none-any.whl size=557487 sha256=8981049e9419ee370dc69a695fad6c36f053eb25eb4a73023d318c008a0ee49f\n",
      "  Stored in directory: /tmp/pip-ephem-wheel-cache-7x2_kudq/wheels/bc/a8/45/819e9fd046e334f10f1c4921cee8054301f73f2faf27a49d4d\n",
      "Successfully built transformers\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Installing collected packages: tokenizers, boto3, filelock, sentencepiece, transformers\n",
      "Successfully installed boto3-1.12.31 filelock-3.0.12 sentencepiece-0.1.82 tokenizers-0.5.2 transformers-2.8.0\n"
     ]
    }
   ],
   "source": [
    "!pip install --no-index /home/guest139/s3transfer/botocore\n",
    "!pip install --no-index /home/guest139/s3transfer\n",
    "!pip install --no-index /home/guest139/sacremoses\n",
    "!pip install --no-index /home/guest139/transformers"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load all files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "\n",
    "en_file1 = '../data/train.lang1.no-punctuation/train.lang1'\n",
    "en_file2 = '../data/train.en.no-punctuation/unaligned.en'\n",
    "fr_file1 = '../data/train.lang2.no-punctuation/train.lang2'\n",
    "fr_file2 = '../data/train.fr.no-punctuation/unaligned.fr'\n",
    "# TODO: exclude the test set\n",
    "\n",
    "# lang_files = [en_file1, en_file2, fr_file1, fr_file2] TODO\n",
    "lang_files = [en_file2]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Tokenize"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Tokenize all files (whole strings (TODO: substrings, capitalization))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import Counter\n",
    "\n",
    "# Add control tokens\n",
    "my_counter = Counter()\n",
    "for i in range(2):\n",
    "    my_counter.update([\"<START>\", \"<STOP>\", \"<UNK>\", \"<MASK>\", \"<SEP>\", \"<PAD>\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "def string_to_tokens(raw_string):\n",
    "    return raw_string.split()\n",
    "\n",
    "line_lengths = []\n",
    "for lang_file in lang_files:\n",
    "    with open(lang_file) as f:\n",
    "        for line in f:\n",
    "            tokens = string_to_tokens(line)\n",
    "            line_lengths += [len(tokens)]\n",
    "            for token in tokens:\n",
    "                my_counter.update([token])    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Keep tokens that occur more than once"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "tokens = np.array(list(my_counter.keys()))\n",
    "frequencies = np.array(list(my_counter.values()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "retained_tokens = tokens[frequencies > 1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "60014\n",
      "40145\n"
     ]
    }
   ],
   "source": [
    "print(len(tokens))\n",
    "print(len(retained_tokens))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Create lookup table dict: string -> token"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "indices = np.array(range(len(retained_tokens)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([    0,     1,     2, ..., 40142, 40143, 40144])"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "indices"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "my_tokenizer_lut = dict(zip(retained_tokens,indices))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Define functions for tokenizing, padding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[    0,     2,   292,  3228,  6307,  2514,   995,     7, 12783,\n",
       "            2,     1,     5,     5,     5,     5,     5,     5,     5,\n",
       "            5,     5,     5,     5,     5,     5,     5,     5,     5,\n",
       "            5,     5,     5,     5,     5]])"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def pad_tokens(token_list, max_length):\n",
    "    if len(token_list) >= max_length:\n",
    "        token_list = token_list[:max_length]\n",
    "        token_list[(max_length-1)] = my_tokenizer_lut[\"<STOP>\"]\n",
    "    else:\n",
    "        while len(token_list) < max_length:\n",
    "            token_list = token_list + [my_tokenizer_lut[\"<PAD>\"]]\n",
    "    return token_list\n",
    "\n",
    "SENTENCE_LENGTH = 32\n",
    "def tokenize_string(raw_string, max_length=SENTENCE_LENGTH): # TODO: Better definition of sentence length\n",
    "    token_list = [my_tokenizer_lut[\"<START>\"]]\n",
    "    \n",
    "    for token in string_to_tokens(raw_string):\n",
    "        if token in my_tokenizer_lut:\n",
    "            token_list += [my_tokenizer_lut[token]]\n",
    "        else:\n",
    "            token_list += [my_tokenizer_lut[\"<UNK>\"]]\n",
    "    \n",
    "    token_list += [my_tokenizer_lut[\"<STOP>\"]]\n",
    "    \n",
    "    token_list = pad_tokens(token_list, max_length)\n",
    "    \n",
    "    return np.array(token_list)[None,:]\n",
    "        \n",
    "tokenize_string(\"The quick brown fox jumped over the lazy dog.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "for the second phase of the trials we just had different sizes small medium large and extra - large it 's true\n",
      "\n",
      "geng had been my host the previous january when i was the first us defense secretary to visit china acting as an interlocutor for president jimmy carter ’s administration\n",
      "\n",
      "so too does the idea that accommodating religious differences is dangerous\n",
      "\n",
      "mr president ladies and gentlemen the financial perspective outlines the scope of the eu ’s activities over coming years as well as providing a framework for such activities and determining how effective they will be\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(474000, 32)"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def tokenize_file(filename):\n",
    "    with open(filename) as f:\n",
    "        tokens = []\n",
    "        for idx, line in enumerate(f):\n",
    "            if idx < 2:\n",
    "                print(line)\n",
    "            tokens += [tokenize_string(line)]\n",
    "    return np.concatenate(tokens,axis=0)\n",
    "\n",
    "x_true = tokenize_file(en_file2)\n",
    "x_true_val = tokenize_file(en_file1)\n",
    "x_true.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Define masking strategy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([ 0,  6,  7,  8,  9, 10,  7, 11, 12, 13, 14, 15, 16,  3, 18, 19, 20,\n",
       "       21, 22, 19, 23, 24, 25,  1,  5,  5,  5,  5,  5,  5,  5,  5])"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def mask_tokens(true_tokens):\n",
    "    non_pad_tokens = true_tokens != my_tokenizer_lut[\"<PAD>\"]\n",
    "    random_masking_seed = np.random.uniform(0,1,(SENTENCE_LENGTH,)) * non_pad_tokens\n",
    "    \n",
    "    masking_targets = 0.85 < random_masking_seed # 15%\n",
    "    mask_token_targets = np.logical_and(0.85 < random_masking_seed, random_masking_seed < 0.85 + 0.15*0.8) # 80% of 15%\n",
    "    random_token_targets = np.logical_and(1.0 - 0.1*0.15 < random_masking_seed, random_masking_seed < 1.0) # 10% of 15%\n",
    "    \n",
    "    masked_tokens = true_tokens.copy()\n",
    "    masked_tokens[mask_token_targets] = my_tokenizer_lut[\"<MASK>\"]\n",
    "    masked_tokens[random_token_targets] = np.random.randint(0,len(my_tokenizer_lut),(random_token_targets.sum(),))\n",
    "#     masked_tokens[~mask_token_targets] = my_tokenizer_lut[\"<PAD>\"]\n",
    "\n",
    "    masked_true_tokens = true_tokens.copy()\n",
    "    masked_true_tokens[~masking_targets] = my_tokenizer_lut[\"<PAD>\"]\n",
    "    \n",
    "    attention_mask = true_tokens != my_tokenizer_lut[\"<PAD>\"]\n",
    "    \n",
    "    return masked_tokens, masking_targets, masked_true_tokens, attention_mask\n",
    "\n",
    "x_train, targets_train, masked_x_true, attention_mask = mask_tokens(x_true)\n",
    "x_train_val, targets_train_val, masked_x_true_val, attention_mask_val = mask_tokens(x_true_val)\n",
    "x_train[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[ True, False, False, False, False, False, False, False, False,\n",
       "        False, False, False, False,  True, False, False,  True, False,\n",
       "        False, False, False, False, False, False, False, False, False,\n",
       "        False, False, False, False, False],\n",
       "       [ True, False, False, False, False, False, False, False, False,\n",
       "        False, False, False, False,  True, False, False,  True, False,\n",
       "        False, False, False, False, False, False, False, False, False,\n",
       "        False, False, False, False, False]])"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "targets_train[:2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[ True,  True,  True,  True,  True,  True,  True,  True,  True,\n",
       "         True,  True,  True,  True,  True,  True,  True,  True,  True,\n",
       "         True,  True,  True,  True,  True,  True, False, False, False,\n",
       "        False, False, False, False, False],\n",
       "       [ True,  True,  True,  True,  True,  True,  True,  True,  True,\n",
       "         True,  True,  True,  True,  True,  True,  True,  True,  True,\n",
       "         True,  True,  True,  True,  True,  True,  True,  True,  True,\n",
       "         True,  True,  True,  True, False]])"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "attention_mask[:2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[ 0,  6,  7,  8,  9, 10,  7, 11, 12, 13, 14, 15, 16, 17, 18, 19,\n",
       "        20, 21, 22, 19, 23, 24, 25,  1,  5,  5,  5,  5,  5,  5,  5,  5],\n",
       "       [ 0,  2, 14, 26, 27, 28,  7, 29, 30, 31, 32, 33,  7, 34, 35, 36,\n",
       "        37, 38, 39, 40, 41, 42, 43, 44,  6, 45, 46, 47, 48, 49,  1,  5]])"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x_true[:2]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Override model to include masking"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "I0407 17:18:30.780511 46997179429888 file_utils.py:57] TensorFlow version 2.1.0 available.\n"
     ]
    }
   ],
   "source": [
    "from transformers import TFBertForMaskedLM, BertConfig"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Overriding functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "class bert_with_mask(tf.keras.Model):\n",
    "    def __init__(self, config, onehot_mask):\n",
    "        super(bert_with_mask, self).__init__()\n",
    "        self.bert = TFBertForMaskedLM(config)\n",
    "        self.onehot_mask = onehot_mask\n",
    "\n",
    "    def call(self, inputs):\n",
    "        mask = inputs[-1] # unpack mask from inputs\n",
    "        inputs = inputs[:-1]\n",
    "        outputs = self.bert(inputs)[0]\n",
    "        \n",
    "        outputs = tf.where(mask[:,:,None], outputs, self.onehot_mask[None,None,:])\n",
    "        \n",
    "        return (outputs,)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Define masking function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "onehot_mask = np.zeros(len(my_tokenizer_lut), dtype=np.float32)\n",
    "onehot_mask[my_tokenizer_lut[\"<PAD>\"]] = 1.0"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Test the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "I0407 17:18:41.834565 46997179429888 configuration_utils.py:281] loading configuration file ../code/bert_config_tiny.json\n",
      "I0407 17:18:41.837863 46997179429888 configuration_utils.py:319] Model config BertConfig {\n",
      "  \"_num_labels\": 2,\n",
      "  \"architectures\": null,\n",
      "  \"attention_probs_dropout_prob\": 0.1,\n",
      "  \"bad_words_ids\": null,\n",
      "  \"bos_token_id\": null,\n",
      "  \"decoder_start_token_id\": null,\n",
      "  \"do_sample\": false,\n",
      "  \"early_stopping\": false,\n",
      "  \"eos_token_id\": null,\n",
      "  \"finetuning_task\": null,\n",
      "  \"hidden_act\": \"gelu\",\n",
      "  \"hidden_dropout_prob\": 0.1,\n",
      "  \"hidden_size\": 128,\n",
      "  \"id2label\": {\n",
      "    \"0\": \"LABEL_0\",\n",
      "    \"1\": \"LABEL_1\"\n",
      "  },\n",
      "  \"initializer_range\": 0.02,\n",
      "  \"intermediate_size\": 512,\n",
      "  \"is_decoder\": false,\n",
      "  \"is_encoder_decoder\": false,\n",
      "  \"label2id\": {\n",
      "    \"LABEL_0\": 0,\n",
      "    \"LABEL_1\": 1\n",
      "  },\n",
      "  \"layer_norm_eps\": 1e-12,\n",
      "  \"length_penalty\": 1.0,\n",
      "  \"max_length\": 20,\n",
      "  \"max_position_embeddings\": 512,\n",
      "  \"min_length\": 0,\n",
      "  \"model_type\": \"bert\",\n",
      "  \"no_repeat_ngram_size\": 0,\n",
      "  \"num_attention_heads\": 2,\n",
      "  \"num_beams\": 1,\n",
      "  \"num_hidden_layers\": 2,\n",
      "  \"num_return_sequences\": 1,\n",
      "  \"output_attentions\": false,\n",
      "  \"output_hidden_states\": false,\n",
      "  \"output_past\": true,\n",
      "  \"pad_token_id\": 0,\n",
      "  \"prefix\": null,\n",
      "  \"pruned_heads\": {},\n",
      "  \"repetition_penalty\": 1.0,\n",
      "  \"task_specific_params\": null,\n",
      "  \"temperature\": 1.0,\n",
      "  \"top_k\": 50,\n",
      "  \"top_p\": 1.0,\n",
      "  \"torchscript\": false,\n",
      "  \"type_vocab_size\": 2,\n",
      "  \"use_bfloat16\": false,\n",
      "  \"vocab_size\": 30522\n",
      "}\n",
      "\n"
     ]
    }
   ],
   "source": [
    "config = BertConfig.from_pretrained('../code/bert_config_tiny.json')\n",
    "config.vocab_size = len(my_tokenizer_lut)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "model2 = bert_with_mask(config, onehot_mask)\n",
    "\n",
    "optimizer = tf.keras.optimizers.Adam(learning_rate=3e-5, epsilon=1e-08, clipnorm=1.0)\n",
    "loss = tf.keras.losses.SparseCategoricalCrossentropy(from_logits=True)\n",
    "\n",
    "model2.compile(optimizer=optimizer, loss=loss)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Define a dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "BATCH_SIZE = 32\n",
    "def data_generator_fn():\n",
    "    for x in x_true:\n",
    "        x_train, targets_train, masked_x_true, attention_mask = mask_tokens(x)\n",
    "        yield (x_train, attention_mask, targets_train), masked_x_true\n",
    "\n",
    "# dataset object\n",
    "dataset = tf.data.Dataset.from_generator(\n",
    "    data_generator_fn,\n",
    "    output_types=((tf.int32, tf.bool, tf.bool), tf.int32),\n",
    "    output_shapes=(( tf.TensorShape((SENTENCE_LENGTH,)), tf.TensorShape((SENTENCE_LENGTH,)), tf.TensorShape((SENTENCE_LENGTH,)) ), tf.TensorShape((SENTENCE_LENGTH,)) )\n",
    ")\n",
    "dataset = dataset.batch(BATCH_SIZE).prefetch(tf.data.experimental.AUTOTUNE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "BATCH_SIZE = 32\n",
    "def data_generator_fn_val():\n",
    "    for x in x_true_val:\n",
    "        x_train, targets_train, masked_x_true, attention_mask = mask_tokens(x)\n",
    "        yield (x_train, attention_mask, targets_train), masked_x_true\n",
    "\n",
    "# dataset object\n",
    "dataset_val = tf.data.Dataset.from_generator(\n",
    "    data_generator_fn_val,\n",
    "    output_types=((tf.int32, tf.bool, tf.bool), tf.int32),\n",
    "    output_shapes=(( tf.TensorShape((SENTENCE_LENGTH,)), tf.TensorShape((SENTENCE_LENGTH,)), tf.TensorShape((SENTENCE_LENGTH,)) ), tf.TensorShape((SENTENCE_LENGTH,)) )\n",
    ")\n",
    "dataset_val = dataset_val.batch(BATCH_SIZE).prefetch(tf.data.experimental.AUTOTUNE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/localscratch/guest139.356254.0/lib/python3.7/site-packages/tensorflow_core/python/framework/indexed_slices.py:433: UserWarning: Converting sparse IndexedSlices to a dense Tensor of unknown shape. This may consume a large amount of memory.\n",
      "  \"Converting sparse IndexedSlices to a dense Tensor of unknown shape. \"\n",
      "W0407 17:21:43.190136 46997179429888 optimizer_v2.py:1043] Gradients do not exist for variables ['bert_with_mask/tf_bert_for_masked_lm/bert/pooler/dense/kernel:0', 'bert_with_mask/tf_bert_for_masked_lm/bert/pooler/dense/bias:0'] when minimizing the loss.\n",
      "/localscratch/guest139.356254.0/lib/python3.7/site-packages/tensorflow_core/python/framework/indexed_slices.py:433: UserWarning: Converting sparse IndexedSlices to a dense Tensor of unknown shape. This may consume a large amount of memory.\n",
      "  \"Converting sparse IndexedSlices to a dense Tensor of unknown shape. \"\n",
      "W0407 17:21:45.591462 46997179429888 optimizer_v2.py:1043] Gradients do not exist for variables ['bert_with_mask/tf_bert_for_masked_lm/bert/pooler/dense/kernel:0', 'bert_with_mask/tf_bert_for_masked_lm/bert/pooler/dense/bias:0'] when minimizing the loss.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "14813/14813 [==============================] - 1405s 95ms/step - loss: 9.2829 - val_loss: 9.2414\n",
      "Epoch 2/10\n",
      " 2064/14813 [===>..........................] - ETA: 20:01 - loss: 9.2382"
     ]
    }
   ],
   "source": [
    "hist = model2.fit(dataset, validation_data=dataset_val, epochs=10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "my_input = tokenize_string(\"I drove to <MASK> .\")\n",
    "my_output = model2((my_input, tf.ones(my_input.shape[:2], dtype=tf.bool) ))[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tf.nn.top_k(my_output[0,4,:], 10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "inv_tokenizer_lut = {v: k for k, v in my_tokenizer_lut.items()}\n",
    "for t in range(7):\n",
    "    print()\n",
    "    print(f\"t{t}:\")\n",
    "    for i in tf.nn.top_k(my_output[0,t,:], 5).indices.numpy():\n",
    "        print(inv_tokenizer_lut[i])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Save/Load model weights"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!rm -rf tinyBERT\n",
    "!mkdir tinyBERT\n",
    "model2.save_weights('tinyBERT/tinyBERT', save_format='tf')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Recreate the model\n",
    "new_model = bert_with_mask(config, onehot_mask)\n",
    "new_model.compile(optimizer=optimizer, loss=loss)\n",
    "\n",
    "# This initializes the variables used by the optimizers,\n",
    "# as well as any stateful metric variables\n",
    "new_model.train_on_batch(dataset.take(1))\n",
    "\n",
    "# Load the state of the old model\n",
    "new_model.load_weights('tinyBERT/tinyBERT')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "my_input = tokenize_string(\"i drove to work\")\n",
    "my_output = new_model((my_input, tf.ones(my_input.shape[:2], dtype=tf.bool) ))[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in range(7):\n",
    "    print(inv_tokenizer_lut[my_input[0,i]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for t in range(7):\n",
    "    print()\n",
    "    print(f\"t{t}:\")\n",
    "    for i, pct in zip(tf.nn.top_k(my_output[0,t,:], 2).indices.numpy(), tf.nn.top_k(my_output[0,t,:], 2).values.numpy()):\n",
    "        print(f\"Guess: {inv_tokenizer_lut[i]}, pct: {pct}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
