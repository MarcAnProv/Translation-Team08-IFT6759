{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "en_file1 = '../data/train.lang1.no-punctuation/train.lang1'\n",
    "en_file2 = '../data/train.en.no-punctuation/unaligned.en'\n",
    "fr_file1 = '../data/train.lang2.no-punctuation/train.lang2'\n",
    "fr_file2 = '../data/train.fr.no-punctuation/unaligned.fr'\n",
    "# TODO: exclude the test set\n",
    "\n",
    "lang_files = [en_file1, en_file2, fr_file1, fr_file2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import Counter\n",
    "\n",
    "# Add control tokens\n",
    "my_counter = Counter()\n",
    "for i in range(2):\n",
    "    my_counter.update([\"<START>\", \"<STOP>\", \"<UNK>\", \"<MASK>\", \"<SEP>\", \"<PAD>\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def string_to_tokens(raw_string):\n",
    "    return raw_string.split()\n",
    "\n",
    "line_lengths = []\n",
    "for lang_file in lang_files:\n",
    "    with open(lang_file) as f:\n",
    "        for line in f:\n",
    "            tokens = string_to_tokens(line)\n",
    "            line_lengths += [len(tokens)]\n",
    "            for token in tokens:\n",
    "                my_counter.update([token])    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "tokens = np.array(list(my_counter.keys()))\n",
    "frequencies = np.array(list(my_counter.values()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "retained_tokens = tokens[frequencies > 1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "127878\n",
      "86384\n"
     ]
    }
   ],
   "source": [
    "print(len(tokens))\n",
    "print(len(retained_tokens))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "indices = np.array(range(len(retained_tokens)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([    0,     1,     2, ..., 86381, 86382, 86383])"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "indices"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "my_tokenizer_lut = dict(zip(retained_tokens,indices))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[   0,    2, 2018, 2659, 1169, 8195,   30,    9, 1244,    2,    1,\n",
       "           5,    5,    5,    5,    5,    5,    5,    5,    5,    5,    5,\n",
       "           5,    5,    5,    5,    5,    5,    5,    5,    5,    5]])"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def pad_tokens(token_list, max_length):\n",
    "    if len(token_list) >= max_length:\n",
    "        token_list = token_list[:max_length]\n",
    "        token_list[(max_length-1)] = my_tokenizer_lut[\"<STOP>\"]\n",
    "    else:\n",
    "        while len(token_list) < max_length:\n",
    "            token_list = token_list + [my_tokenizer_lut[\"<PAD>\"]]\n",
    "    return token_list\n",
    "\n",
    "def tokenize_string(raw_string, max_length=32): # TODO: Better definition of sentence length\n",
    "    token_list = [my_tokenizer_lut[\"<START>\"]]\n",
    "    \n",
    "    for token in string_to_tokens(raw_string):\n",
    "        if token in my_tokenizer_lut:\n",
    "            token_list += [my_tokenizer_lut[token]]\n",
    "        else:\n",
    "            token_list += [my_tokenizer_lut[\"<UNK>\"]]\n",
    "    \n",
    "    token_list += [my_tokenizer_lut[\"<STOP>\"]]\n",
    "    \n",
    "    token_list = pad_tokens(token_list, max_length)\n",
    "    \n",
    "    return np.array(token_list)[None,:]\n",
    "        \n",
    "tokenize_string(\"The quick brown fox jumped over the lazy dog.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(11000, 32)"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def tokenize_file(filename):\n",
    "    with open(filename) as f:\n",
    "        tokens = []\n",
    "        for line in f:\n",
    "            tokens += [tokenize_string(line)]\n",
    "    return np.concatenate(tokens,axis=0)\n",
    "\n",
    "x_true = tokenize_file(en_file1)\n",
    "x_true.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([ 0,  6,  7,  8,  9, 10, 11, 12, 13,  3, 15, 16,  1,  5,  5,  5,  5,\n",
       "        5,  5,  5,  5,  5,  5,  5,  5,  5,  5,  5,  5,  5,  5,  5])"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def mask_tokens(true_tokens):\n",
    "    non_pad_tokens = true_tokens != my_tokenizer_lut[\"<PAD>\"]\n",
    "    random_masking_seed = np.random.uniform(0,1,(32,)) * non_pad_tokens\n",
    "    \n",
    "    masking_targets = 0.85 < random_masking_seed # 15% # TODO: Use masking targets\n",
    "    mask_token_targets = np.logical_and(0.85 < random_masking_seed, random_masking_seed < 0.85 + 0.15*0.8) # 80% of 15%\n",
    "    random_token_targets = np.logical_and(1.0 - 0.1*0.15 < random_masking_seed, random_masking_seed < 1.0) # 10% of 15%\n",
    "    \n",
    "    masked_tokens = true_tokens.copy()\n",
    "    masked_tokens[mask_token_targets] = my_tokenizer_lut[\"<MASK>\"]\n",
    "    masked_tokens[random_token_targets] = np.random.randint(0,len(my_tokenizer_lut),(random_token_targets.sum(),))\n",
    "    \n",
    "    return masked_tokens\n",
    "\n",
    "x_train = mask_tokens(x_true)\n",
    "x_train[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([ True,  True,  True,  True,  True,  True,  True,  True,  True,\n",
       "        True,  True,  True,  True, False, False, False, False, False,\n",
       "       False, False, False, False, False, False, False, False, False,\n",
       "       False, False, False, False, False])"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "attention_mask = x_train != my_tokenizer_lut[\"<PAD>\"]\n",
    "attention_mask[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([ 0,  6,  7,  8,  9, 10, 11, 12, 13, 14, 15, 16,  1,  5,  5,  5,  5,\n",
       "        5,  5,  5,  5,  5,  5,  5,  5,  5,  5,  5,  5,  5,  5,  5])"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x_true[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import BertConfig, TFBertForMaskedLM\n",
    "config = BertConfig.from_pretrained('../uncased_L-2_H-128_A-2/bert_config_tiny.json')\n",
    "config.vocab_size = len(my_tokenizer_lut)\n",
    "model = TFBertForMaskedLM(config)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(<tf.Tensor: shape=(1, 32, 86384), dtype=float32, numpy=\n",
       " array([[[ 0.09791588,  0.13342728, -0.10703319, ..., -0.16127932,\n",
       "           0.03996952,  0.16378266],\n",
       "         [-0.04859214,  0.06760185,  0.04196171, ..., -0.1572957 ,\n",
       "           0.13940057,  0.11954289],\n",
       "         [ 0.28934202, -0.15358104,  0.14815764, ..., -0.129953  ,\n",
       "           0.16664478, -0.0650624 ],\n",
       "         ...,\n",
       "         [ 0.04984542, -0.04067166, -0.11882814, ..., -0.1577919 ,\n",
       "           0.32993722, -0.09779587],\n",
       "         [ 0.11055933, -0.20321994,  0.1207041 , ..., -0.332794  ,\n",
       "          -0.02643372, -0.06331111],\n",
       "         [ 0.10005414, -0.18562782,  0.24596229, ..., -0.3783547 ,\n",
       "          -0.07882662,  0.02330005]]], dtype=float32)>,)"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "input_ids = tf.constant(tokenize_string(\"Hello, my dog is cute\"))\n",
    "outputs = model((input_ids,))\n",
    "outputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "86384"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(my_tokenizer_lut)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tf.Tensor: shape=(1, 32), dtype=int64, numpy=\n",
       "array([[   0,    2,   55, 1695,   15, 9325,    1,    5,    5,    5,    5,\n",
       "           5,    5,    5,    5,    5,    5,    5,    5,    5,    5,    5,\n",
       "           5,    5,    5,    5,    5,    5,    5,    5,    5,    5]])>"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "input_ids"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 11000 samples\n",
      "WARNING:tensorflow:Gradients do not exist for variables ['tf_bert_for_masked_lm/bert/pooler/dense/kernel:0', 'tf_bert_for_masked_lm/bert/pooler/dense/bias:0'] when minimizing the loss.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/alex/tr_env/lib/python3.6/site-packages/tensorflow_core/python/framework/indexed_slices.py:433: UserWarning: Converting sparse IndexedSlices to a dense Tensor of unknown shape. This may consume a large amount of memory.\n",
      "  \"Converting sparse IndexedSlices to a dense Tensor of unknown shape. \"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:Gradients do not exist for variables ['tf_bert_for_masked_lm/bert/pooler/dense/kernel:0', 'tf_bert_for_masked_lm/bert/pooler/dense/bias:0'] when minimizing the loss.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/alex/tr_env/lib/python3.6/site-packages/tensorflow_core/python/framework/indexed_slices.py:433: UserWarning: Converting sparse IndexedSlices to a dense Tensor of unknown shape. This may consume a large amount of memory.\n",
      "  \"Converting sparse IndexedSlices to a dense Tensor of unknown shape. \"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "11000/11000 [==============================] - 373s 34ms/sample - loss: 9.6819\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<tensorflow.python.keras.callbacks.History at 0x7fa1d8368438>"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "optimizer = tf.keras.optimizers.Adam(learning_rate=3e-5, epsilon=1e-08, clipnorm=1.0)\n",
    "loss = tf.keras.losses.SparseCategoricalCrossentropy(from_logits=True)\n",
    "\n",
    "model.compile(optimizer=optimizer, loss=loss)\n",
    "model.fit((x_train,attention_mask), x_true)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "my_input = tokenize_string(\"I drove to <MASK>\")\n",
    "my_output = model(my_input)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[    0,     2, 10439,   119,     3,     1,     5,     5,     5,\n",
       "            5,     5,     5,     5,     5,     5,     5,     5,     5,\n",
       "            5,     5,     5,     5,     5,     5,     5,     5,     5,\n",
       "            5,     5,     5,     5,     5]])"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "my_input"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tf.Tensor: shape=(10,), dtype=float32, numpy=\n",
       "array([ 1.4085811 ,  1.6936857 ,  0.54693615, -0.08038695, -1.3574069 ,\n",
       "        2.6084661 ,  0.86079395,  0.1713116 ,  0.7250719 ,  1.8401847 ],\n",
       "      dtype=float32)>"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# tf.argmax(my_output[0][0,4,:])\n",
    "my_output[0][0,4,:][:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "inv_tokenizer_lut = {v: k for k, v in my_tokenizer_lut.items()}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'<PAD>'"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "inv_tokenizer_lut[5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "953"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "my_tokenizer_lut['work']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
