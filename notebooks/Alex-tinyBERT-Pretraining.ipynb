{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Intialize"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load all files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "\n",
    "en_file1 = '../data/train.lang1.no-punctuation/train.lang1'\n",
    "en_file2 = '../data/train.en.no-punctuation/unaligned.en'\n",
    "fr_file1 = '../data/train.lang2.no-punctuation/train.lang2'\n",
    "fr_file2 = '../data/train.fr.no-punctuation/unaligned.fr'\n",
    "# TODO: exclude the test set\n",
    "\n",
    "lang_files = [en_file1, en_file2, fr_file1, fr_file2]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Tokenize"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Tokenize all files (whole strings (TODO: substrings, capitalization))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import Counter\n",
    "\n",
    "# Add control tokens\n",
    "my_counter = Counter()\n",
    "for i in range(2):\n",
    "    my_counter.update([\"<START>\", \"<STOP>\", \"<UNK>\", \"<MASK>\", \"<SEP>\", \"<PAD>\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def string_to_tokens(raw_string):\n",
    "    return raw_string.split()\n",
    "\n",
    "line_lengths = []\n",
    "for lang_file in lang_files:\n",
    "    with open(lang_file) as f:\n",
    "        for line in f:\n",
    "            tokens = string_to_tokens(line)\n",
    "            line_lengths += [len(tokens)]\n",
    "            for token in tokens:\n",
    "                my_counter.update([token])    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Keep tokens that occur more than once"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "tokens = np.array(list(my_counter.keys()))\n",
    "frequencies = np.array(list(my_counter.values()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "retained_tokens = tokens[frequencies > 1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "127878\n",
      "86384\n"
     ]
    }
   ],
   "source": [
    "print(len(tokens))\n",
    "print(len(retained_tokens))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Create lookup table dict: string -> token"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "indices = np.array(range(len(retained_tokens)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([    0,     1,     2, ..., 86381, 86382, 86383])"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "indices"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "my_tokenizer_lut = dict(zip(retained_tokens,indices))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Define functions for tokenizing, padding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[   0,    2, 2018, 2659, 1169, 8195,   30,    9, 1244,    2,    1,\n",
       "           5,    5,    5,    5,    5,    5,    5,    5,    5,    5,    5,\n",
       "           5,    5,    5,    5,    5,    5,    5,    5,    5,    5]])"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def pad_tokens(token_list, max_length):\n",
    "    if len(token_list) >= max_length:\n",
    "        token_list = token_list[:max_length]\n",
    "        token_list[(max_length-1)] = my_tokenizer_lut[\"<STOP>\"]\n",
    "    else:\n",
    "        while len(token_list) < max_length:\n",
    "            token_list = token_list + [my_tokenizer_lut[\"<PAD>\"]]\n",
    "    return token_list\n",
    "\n",
    "def tokenize_string(raw_string, max_length=32): # TODO: Better definition of sentence length\n",
    "    token_list = [my_tokenizer_lut[\"<START>\"]]\n",
    "    \n",
    "    for token in string_to_tokens(raw_string):\n",
    "        if token in my_tokenizer_lut:\n",
    "            token_list += [my_tokenizer_lut[token]]\n",
    "        else:\n",
    "            token_list += [my_tokenizer_lut[\"<UNK>\"]]\n",
    "    \n",
    "    token_list += [my_tokenizer_lut[\"<STOP>\"]]\n",
    "    \n",
    "    token_list = pad_tokens(token_list, max_length)\n",
    "    \n",
    "    return np.array(token_list)[None,:]\n",
    "        \n",
    "tokenize_string(\"The quick brown fox jumped over the lazy dog.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "so too does the idea that accommodating religious differences is dangerous\n",
      "\n",
      "mr president ladies and gentlemen the financial perspective outlines the scope of the eu â€™s activities over coming years as well as providing a framework for such activities and determining how effective they will be\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(11000, 32)"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def tokenize_file(filename):\n",
    "    with open(filename) as f:\n",
    "        tokens = []\n",
    "        for idx, line in enumerate(f):\n",
    "            if idx < 2:\n",
    "                print(line)\n",
    "            tokens += [tokenize_string(line)]\n",
    "    return np.concatenate(tokens,axis=0)\n",
    "\n",
    "x_true = tokenize_file(en_file1)\n",
    "x_true.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Define masking strategy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([ 0,  6,  7,  8,  9,  3, 11,  3, 13, 14, 15, 16,  1,  5,  5,  5,  5,\n",
       "        5,  5,  5,  5,  5,  5,  5,  5,  5,  5,  5,  5,  5,  5,  5])"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def mask_tokens(true_tokens):\n",
    "    non_pad_tokens = true_tokens != my_tokenizer_lut[\"<PAD>\"]\n",
    "    random_masking_seed = np.random.uniform(0,1,(32,)) * non_pad_tokens\n",
    "    \n",
    "    masking_targets = 0.85 < random_masking_seed # 15%\n",
    "    mask_token_targets = np.logical_and(0.85 < random_masking_seed, random_masking_seed < 0.85 + 0.15*0.8) # 80% of 15%\n",
    "    random_token_targets = np.logical_and(1.0 - 0.1*0.15 < random_masking_seed, random_masking_seed < 1.0) # 10% of 15%\n",
    "    \n",
    "    masked_tokens = true_tokens.copy()\n",
    "    masked_tokens[mask_token_targets] = my_tokenizer_lut[\"<MASK>\"]\n",
    "    masked_tokens[random_token_targets] = np.random.randint(0,len(my_tokenizer_lut),(random_token_targets.sum(),))\n",
    "#     masked_tokens[~mask_token_targets] = my_tokenizer_lut[\"<PAD>\"]\n",
    "    \n",
    "    return masked_tokens, masking_targets\n",
    "\n",
    "x_train, targets_train = mask_tokens(x_true)\n",
    "x_train[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[False, False, False, False, False,  True, False,  True, False,\n",
       "        False, False, False, False, False, False, False, False, False,\n",
       "        False, False, False, False, False, False, False, False, False,\n",
       "        False, False, False, False, False],\n",
       "       [False, False, False, False, False,  True, False,  True, False,\n",
       "        False, False, False, False, False, False, False, False,  True,\n",
       "        False, False, False, False,  True, False, False, False,  True,\n",
       "        False, False, False, False, False]])"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "targets_train[:2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[ True,  True,  True,  True,  True,  True,  True,  True,  True,\n",
       "         True,  True,  True,  True, False, False, False, False, False,\n",
       "        False, False, False, False, False, False, False, False, False,\n",
       "        False, False, False, False, False],\n",
       "       [ True,  True,  True,  True,  True,  True,  True,  True,  True,\n",
       "         True,  True,  True,  True,  True,  True,  True,  True,  True,\n",
       "         True,  True,  True,  True,  True,  True,  True,  True,  True,\n",
       "         True,  True,  True,  True,  True]])"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "attention_mask = x_train != my_tokenizer_lut[\"<PAD>\"]\n",
    "attention_mask[:2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[ 0,  6,  7,  8,  9, 10, 11, 12, 13, 14, 15, 16,  1,  5,  5,  5,\n",
       "         5,  5,  5,  5,  5,  5,  5,  5,  5,  5,  5,  5,  5,  5,  5,  5],\n",
       "       [ 0, 17, 18, 19, 20, 21,  9, 22, 23, 24,  9, 25, 26,  9, 27, 28,\n",
       "        29, 30, 31, 32, 33, 34, 33, 35, 36, 37, 38, 39, 29, 20, 40,  1]])"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x_true[:2]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Override model to include masking"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import TFBertForMaskedLM, BertConfig"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Overriding functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "class bert_with_mask(tf.keras.Model):\n",
    "    def __init__(self, config, onehot_mask):\n",
    "        super(bert_with_mask, self).__init__()\n",
    "        self.bert = TFBertForMaskedLM(config)\n",
    "        self.onehot_mask = onehot_mask\n",
    "\n",
    "    def call(self, inputs):\n",
    "        mask = inputs[-1] # unpack mask from inputs\n",
    "        inputs = inputs[:-1]\n",
    "        outputs = self.bert(inputs)[0]\n",
    "        \n",
    "        outputs = tf.where(mask[:,:,None], outputs, self.onehot_mask[None,None,:])\n",
    "        \n",
    "        return (outputs,)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Define masking function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "onehot_mask = np.zeros(len(my_tokenizer_lut), dtype=np.float32)\n",
    "onehot_mask[my_tokenizer_lut[\"<PAD>\"]] = 1.0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_true[~targets_train] = my_tokenizer_lut[\"<PAD>\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Test the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "config = BertConfig.from_pretrained('../code/bert_config_tiny.json')\n",
    "config.vocab_size = len(my_tokenizer_lut)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "model2 = bert_with_mask(config, onehot_mask)\n",
    "\n",
    "optimizer = tf.keras.optimizers.Adam(learning_rate=3e-5, epsilon=1e-08, clipnorm=1.0)\n",
    "# optimizer = tf.keras.optimizers.Adam(learning_rate=0.1, epsilon=1e-07, clipnorm=1.0)\n",
    "loss = tf.keras.losses.SparseCategoricalCrossentropy(from_logits=True)\n",
    "\n",
    "model2.compile(optimizer=optimizer, loss=loss)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 11000 samples\n",
      "WARNING:tensorflow:Gradients do not exist for variables ['bert_with_mask/tf_bert_for_masked_lm/bert/pooler/dense/kernel:0', 'bert_with_mask/tf_bert_for_masked_lm/bert/pooler/dense/bias:0'] when minimizing the loss.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/alex/tr_env/lib/python3.6/site-packages/tensorflow_core/python/framework/indexed_slices.py:433: UserWarning: Converting sparse IndexedSlices to a dense Tensor of unknown shape. This may consume a large amount of memory.\n",
      "  \"Converting sparse IndexedSlices to a dense Tensor of unknown shape. \"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:Gradients do not exist for variables ['bert_with_mask/tf_bert_for_masked_lm/bert/pooler/dense/kernel:0', 'bert_with_mask/tf_bert_for_masked_lm/bert/pooler/dense/bias:0'] when minimizing the loss.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/alex/tr_env/lib/python3.6/site-packages/tensorflow_core/python/framework/indexed_slices.py:433: UserWarning: Converting sparse IndexedSlices to a dense Tensor of unknown shape. This may consume a large amount of memory.\n",
      "  \"Converting sparse IndexedSlices to a dense Tensor of unknown shape. \"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "11000/11000 [==============================] - 424s 39ms/sample - loss: 10.3863\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<tensorflow.python.keras.callbacks.History at 0x7f3b0ee39240>"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Train on full dataset\n",
    "model2.fit((x_train,attention_mask,targets_train), x_true)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "# my_input = tokenize_string(\"<MASK> can't be choosers.\")\n",
    "my_input = tokenize_string(\"I drove to <MASK> .\")\n",
    "my_output = model2((my_input, tf.ones(my_input.shape[:2], dtype=tf.bool) ))[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "TopKV2(values=<tf.Tensor: shape=(10,), dtype=float32, numpy=\n",
       "array([2.5499732, 1.7648752, 1.5802052, 1.561729 , 1.5576662, 1.4899919,\n",
       "       1.4283481, 1.3566295, 1.3235406, 1.1286547], dtype=float32)>, indices=<tf.Tensor: shape=(10,), dtype=int32, numpy=array([  9,   1,  15, 119,  26, 108,  20,  11,  82, 178], dtype=int32)>)"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tf.nn.top_k(my_output[0,4,:], 10)\n",
    "# tf.nn.top_k(my_output[0,1,:], 5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "the\n",
      "<STOP>\n",
      "is\n",
      "to\n",
      "of\n",
      "in\n",
      "and\n",
      "that\n",
      "are\n",
      "not\n"
     ]
    }
   ],
   "source": [
    "inv_tokenizer_lut = {v: k for k, v in my_tokenizer_lut.items()}\n",
    "for i in [  9,   1,  15, 119,  26, 108,  20,  11,  82, 178]:\n",
    "    print(inv_tokenizer_lut[i])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# TODO: Define a dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ds = tf.data.Dataset.from_tensors(x_true)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def mask_tokens_tf(true_tokens):\n",
    "    non_pad_tokens = true_tokens != my_tokenizer_lut[\"<PAD>\"]\n",
    "    random_masking_seed = np.random.uniform(0,1,(32,)) * non_pad_tokens\n",
    "    \n",
    "    masking_targets = 0.85 < random_masking_seed # 15%\n",
    "    mask_token_targets = np.logical_and(0.85 < random_masking_seed, random_masking_seed < 0.85 + 0.15*0.8) # 80% of 15%\n",
    "    random_token_targets = np.logical_and(1.0 - 0.1*0.15 < random_masking_seed, random_masking_seed < 1.0) # 10% of 15%\n",
    "    \n",
    "    masked_tokens = true_tokens.clone()\n",
    "    masked_tokens[mask_token_targets] = my_tokenizer_lut[\"<MASK>\"]\n",
    "    masked_tokens[random_token_targets] = np.random.randint(0,len(my_tokenizer_lut),(random_token_targets.sum(),))\n",
    "    \n",
    "    return masked_tokens, masking_targets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ds.apply(mask_tokens_tf)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
