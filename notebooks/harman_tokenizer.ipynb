{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "harman_tokenizer.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "mount_file_id": "18TEGLm-JgwXfA08oxbYWCN2AZpyCmHRu",
      "authorship_tag": "ABX9TyOpMy6OtZxLs4BWoxoOm9a+",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/notAlex2/Translation-Team08-IFT6759/blob/master/notebooks/harman_tokenizer.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9NetP1bhU2MS",
        "colab_type": "text"
      },
      "source": [
        "This notebook contains code on how to train Byte-Pair tokenizer on your own dataset. Byte-Pair encoding handles Out-Of-Vocabulary words as well!"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1TrBC_yz4mpj",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import os\n",
        "\n",
        "project_path = \"/content/drive/My Drive/machine-translation\"\n",
        "os.chdir(project_path)\n",
        "\n",
        "data_path = os.path.join(project_path, 'data')"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "H6NNccnH8lTy",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "! pip install transformers===2.7.0 \n",
        "\n",
        "try:\n",
        "    import tokenizers\n",
        "except ImportError as e:\n",
        "    ! pip install tokenizers"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1XNCxDxh8rQ1",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import transformers\n",
        "import tensorflow as tf\n",
        "import io\n",
        "import json\n",
        "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
        "from transformers import (\n",
        "    PreTrainedTokenizer, \n",
        "    AutoTokenizer\n",
        ")\n",
        "\n",
        "\n",
        "def set_seed(seed):\n",
        "    tf.random.set_seed(seed)\n",
        "\n",
        "set_seed(10)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "GXjf4iS0KLOC",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def train_tokenizer(save_tokenizer_path: str,\n",
        "                    training_files: str,\n",
        "                    VOCAB_EXIST: bool,\n",
        "                    VOCAB_SIZE: int) -> None:\n",
        "\n",
        "    # Need to train tokenizer only once\n",
        "    # If already saved tokenizer, set VOCAB_EXIST to True\n",
        "    if not VOCAB_EXIST:\n",
        "        # create vocab from scratch\n",
        "        bpe_tokenizer = tokenizers.ByteLevelBPETokenizer(lowercase=True)\n",
        "        special_tokens = [\"<s>\",\"<pad>\",\"</s>\",\"<unk>\",\"<mask>\"]\n",
        "\n",
        "        bpe_tokenizer.train(\n",
        "                        files=training_files, \n",
        "                        vocab_size=VOCAB_SIZE, \n",
        "                        min_frequency=2, \n",
        "                        show_progress=True,\n",
        "                        special_tokens=special_tokens\n",
        "                        )\n",
        "\n",
        "        if not os.path.exists(save_tokenizer_path):\n",
        "            os.makedirs(save_tokenizer_path)\n",
        "        \n",
        "        # This saves 2 files, which are required later by the tokenizer: merges.txt and vocab.json\n",
        "        bpe_tokenizer.save(save_tokenizer_path)\n",
        "        \n",
        "\n",
        "        model_type=\"roberta\" # roBERTa model is better than BERT for language modelling\n",
        "        config = {\n",
        "            \"vocab_size\": VOCAB_SIZE,\n",
        "            \"model_type\": model_type \n",
        "            }\n",
        "\n",
        "        config_path = os.path.join(save_tokenizer_path, \"config.json\")\n",
        "        with open(config_path, 'w') as fp:\n",
        "            json.dump(config, fp)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ipg3Vi4ZLIvW",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "VOCAB_EXIST = True\n",
        "VOCAB_SIZE = 40000\n",
        "pretrained_tokenizer_path = \"tokenizer_data\"\n",
        "\n",
        "unaligned_en_path = os.path.join(data_path, 'unaligned.en')\n",
        "aligned_en_path = os.path.join(data_path, 'train.lang1')\n",
        "training_files = [unaligned_en_path, aligned_en_path]\n",
        "\n",
        "# train your Byte Pair Tokenizer on your own training files!\n",
        "train_tokenizer(pretrained_tokenizer_path, training_files, VOCAB_EXIST, VOCAB_SIZE)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "gPeYRoGBJGyP",
        "colab_type": "text"
      },
      "source": [
        "### Load Saved Tokenizer"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7_B3j8L4JFwp",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# make sure the path contains 3 files: config.json, merges.txt and vocab.json\n",
        "tokenizer = AutoTokenizer.from_pretrained(pretrained_tokenizer_path, cache_dir=None)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "tFlRH311JJRb",
        "colab_type": "text"
      },
      "source": [
        "### How to use Tokenizer"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "XZf0htpZuwzp",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "80174f91-0ff1-4fe4-c8eb-b382b913e22e"
      },
      "source": [
        "text = \"Montreal is a great city\".strip()\n",
        "tokenizer.tokenize(text)"
      ],
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['M', 'ont', 'real', 'Ġis', 'Ġa', 'Ġgreat', 'Ġcity']"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 7
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "outputId": "dc43afe9-0d23-4e17-c313-8b17bb55fbfc",
        "id": "zNaQ3iQoD9Qx",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "encoded_seq = tokenizer.encode(text)\n",
        "encoded_seq"
      ],
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[0, 225, 49, 2096, 9317, 306, 263, 805, 3195, 2]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 8
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "G09sii-l26VL",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "0e510485-1a50-42e4-95ef-82038d4ba688"
      },
      "source": [
        "# decode sequence back!\n",
        "tokenizer.decode(encoded_seq, skip_special_tokens=False)"
      ],
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'<s> Montreal is a great city</s>'"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 9
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "MFraHntzFJxs",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "4234777e-52b3-4f16-bb35-bcbe34319adf"
      },
      "source": [
        "tokenizer.decode(encoded_seq, skip_special_tokens=True)"
      ],
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "' Montreal is a great city'"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 10
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "rrN-miCauNjI",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 51
        },
        "outputId": "97c9abf6-1ad3-42a1-a6f6-254cc07d9953"
      },
      "source": [
        "tokens = tokenizer.encode_plus(text)\n",
        "tokens"
      ],
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "{'attention_mask': [1, 1, 1, 1, 1, 1, 1, 1, 1, 1],\n",
              " 'input_ids': [0, 225, 49, 2096, 9317, 306, 263, 805, 3195, 2]}"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 11
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-JqhRyIosiJ4",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "25576065-9a86-4799-cfb2-822fe5d4a4fa"
      },
      "source": [
        "tokens[\"input_ids\"]"
      ],
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[0, 225, 49, 2096, 9317, 306, 263, 805, 3195, 2]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 12
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "kxiZ6u9GsiLR",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "a07bce8b-c828-4456-d46a-324fc83e9e45"
      },
      "source": [
        "tokenizer.get_special_tokens_mask(tokens[\"input_ids\"], already_has_special_tokens=True)"
      ],
      "execution_count": 13,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[1, 0, 0, 0, 0, 0, 0, 0, 0, 1]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 13
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "V5WgNfinsiHr",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 51
        },
        "outputId": "a8bea5d9-a8ea-4783-dfc2-61a2b3b17700"
      },
      "source": [
        "# pad sequences!\n",
        "padded_seq = pad_sequences([tokens[\"input_ids\"]], padding='post', value=tokenizer.pad_token_id, maxlen=15)\n",
        "padded_seq[0]"
      ],
      "execution_count": 14,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([   0,  225,   49, 2096, 9317,  306,  263,  805, 3195,    2,    1,\n",
              "          1,    1,    1,    1], dtype=int32)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 14
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "FL0weZm9siGV",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "dee83b90-bdc9-4e97-a24e-993af649a211"
      },
      "source": [
        "tokenizer.get_special_tokens_mask(padded_seq[0], already_has_special_tokens=True)"
      ],
      "execution_count": 15,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[1, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 15
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_IGlV7YcSXEN",
        "colab_type": "text"
      },
      "source": [
        "#### Un-tokenize inputs"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3IDsuZlexIQK",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "e4e64dfe-5c96-44c1-93d0-eaff5b3e8d84"
      },
      "source": [
        "tokenizer.decode(padded_seq[0], skip_special_tokens=False)"
      ],
      "execution_count": 16,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'<s> Montreal is a great city</s><pad><pad><pad><pad><pad>'"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 16
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ldWmAe6oxIKx",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "57e20633-76c7-4bad-ece2-a9a1c59ff2c1"
      },
      "source": [
        "tokenizer.decode(padded_seq[0], skip_special_tokens=True)"
      ],
      "execution_count": 17,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "' Montreal is a great city'"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 17
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "outputId": "11d85391-9926-4b8f-ef7c-17a297fcf1cb",
        "id": "xRZPf7wOJOqM",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 85
        }
      },
      "source": [
        "# encode batch in one go!\n",
        "text1 = \"Montreal is a great city\".strip()\n",
        "text2 = \"California has good weather\".strip()\n",
        "\n",
        "texts = [text1, text2]\n",
        "tokenizer.batch_encode_plus(texts)"
      ],
      "execution_count": 18,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "{'attention_mask': [[1, 1, 1, 1, 1, 1, 1, 1, 1, 1],\n",
              "  [1, 1, 1, 1, 1, 1, 1, 1, 1]],\n",
              " 'input_ids': [[0, 225, 49, 2096, 9317, 306, 263, 805, 3195, 2],\n",
              "  [0, 225, 39, 289, 8955, 407, 793, 5872, 2]]}"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 18
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "GaeRewylSEjR",
        "colab_type": "text"
      },
      "source": [
        "### Use of Tokenizer with `tf.Dataset` Object"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "e2RPQgoi667q",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# datalaoder stuff\n",
        "def tokenize_string(tokenizer, raw_string):\n",
        "    return tokenizer.encode(raw_string)\n",
        "  \n",
        "def encode_dataset(file_path, pretrained_tokenizer_path, num_examples):\n",
        "    tokenizer = AutoTokenizer.from_pretrained(pretrained_tokenizer_path)\n",
        "    sentences = io.open(file_path, encoding='UTF-8').read().strip().split('\\n')\n",
        "    encoded_sequences = [tokenize_string(tokenizer, sentence) for sentence in sentences[:num_examples]]\n",
        "    encoded_sequences = pad_sequences(encoded_sequences, padding='post', value=tokenizer.pad_token_id)\n",
        "    decoded_sequences = [tokenizer.decode(x) for x in encoded_sequences]\n",
        "    return encoded_sequences, decoded_sequences\n",
        "\n",
        "\n",
        "def data_generator_fn():\n",
        "    NUM_EXAMPLES = 10\n",
        "    encoded_sequences, decoded_sequences = encode_dataset(unaligned_en_path, pretrained_tokenizer_path, NUM_EXAMPLES)\n",
        "\n",
        "    for enc_seq, decoded_seq in zip(encoded_sequences, decoded_sequences):\n",
        "        yield enc_seq, decoded_seq\n",
        "\n",
        "BATCH_SIZE = 2\n",
        "# dataset object\n",
        "dataset = tf.data.Dataset.from_generator(\n",
        "    data_generator_fn,\n",
        "    output_types=(tf.int32, tf.string)\n",
        "    ).batch(BATCH_SIZE).prefetch(tf.data.experimental.AUTOTUNE)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8NUVxzlx3-5Z",
        "colab_type": "code",
        "outputId": "b7ad0c7a-37a7-43a7-8da2-af8cbb360e7d",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 275
        }
      },
      "source": [
        "for enc, dec in dataset.take(1):\n",
        "    print(enc)\n",
        "    print(dec)"
      ],
      "execution_count": 20,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "tf.Tensor(\n",
            "[[    0   225    42   280   266  1354  4876   288   266  8427   324   648\n",
            "    728  1102 14787    16  1372    16  3012    16  1397   299  2886    17\n",
            "  13103    18   225    45    88   411  1819    18     2     1     1     1\n",
            "      1     1     1     1     1     1     1     1     1     1     1     1\n",
            "      1     1     1     1     1]\n",
            " [    0   225    43  1443   728   491   544  3766   266  2506   225    46\n",
            "   3454    16   698   225    45   438   266   767   225    57    55  5719\n",
            "   4873   282  3769   225    39    76  1132    16  5122   352   290 22882\n",
            "    325   225    52   357  1798   225    46   377  1326   225    39 13824\n",
            "    547    87  2976    18     2]], shape=(2, 53), dtype=int32)\n",
            "tf.Tensor(\n",
            "[b\"<s> For the second phase of the trials we just had different sizes, small, medium, large and extra-large. It's true.</s><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad>\"\n",
            " b'<s> Geng had been my host the previous January, when I was the first US defense secretary to visit China, acting as an interlocutor for President Jimmy Carter\\xe2\\x80\\x99s administration.</s>'], shape=(2,), dtype=string)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "gT7jHj57IMtL",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    }
  ]
}